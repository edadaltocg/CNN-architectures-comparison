{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cuVR7dGPAdqT"
   },
   "source": [
    "# Comparison of Convolutional Neural Networks (CNN) Architectures\n",
    "\n",
    "This notebook requires a little or moderate experience with CNNs. A great introductory notebook to CNNs is [Deep Learning and Convolutional Neural Networks](https://github.com/erachelson/MLclass/blob/master/7%20-%20Deep%20Learning/Deep%20Learning%20and%20CNN.ipynb) by [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en).\n",
    "\n",
    "This tutorial will use Keras as API - a minimalist, modular neural network library - and Tensorflow as backend. It requires internet connection. Google colab is advised since this notebook uses tensorflow 2.x and train a CNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GCv4TwD4D4hO"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Convolutional Neural Networks (or CNNs or ConvNets) is a family of artificial neural networks (ANN) built basically by stacking Convolutional/Pooling blocks upon each other and adding a final set of dense layers. Even though ANNs and CNNs architectures are very similar, ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network. \n",
    "\n",
    "State of the art ConvNets uses different blocks and connection between layers and are usually very deep, although depth does not correlate with performance. In this notebook, you will get familiar with the principal ConvNets architectures of the literature, as well as how to implement them and how they perform in a single example and in a standardized dataset.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qmIk3icV7vk0"
   },
   "source": [
    "## Summary\n",
    "\n",
    "* Architecture basics\n",
    "* Benchmark and Deep Learning Challenge\n",
    "* Important (and Famous) Architectures\n",
    "* Comparison between Principal Architectures\n",
    "* Analysis\n",
    "* Feature Extraction and Training\n",
    "* Conclusions\n",
    "* Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7vha5yqA7-A0"
   },
   "source": [
    "## I. Architecture Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rRxCSwMuf8vS"
   },
   "source": [
    "\n",
    "### Layers\n",
    "\n",
    "A classical ConvNet is built through a sequence of familiar layers as shown in the list below. Each architecture combine these layers in order to obtain the best performance for a certain class of problems and benchmarks.  \n",
    "\n",
    "* Input Layer (INPUT)\n",
    "* Convolutional Layer (most computationally demanding) (CONV)\n",
    "* Pooling Layer (POOL)\n",
    "* Fully-Connected Layer (FC)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QGHJU1BTf_tV"
   },
   "source": [
    "\n",
    "#### Conv Layer Hyperparameters\n",
    "\n",
    "Usually we want the output volume of the convolutional layer to have the same height and width of the input image. For controlling this, three hyperparameters are important.\n",
    "\n",
    "* Depth ($K$):  corresponds to the number of filters we would like to use, each learning to look for something different in the input. For example, if the first Convolutional Layer takes as input the raw image, then different neurons along the depth dimension may activate in presence of various oriented edges, or blobs of color. \n",
    "* Stride ($S$): the amount the filter slides.  When the stride is 1 then we move the filters one pixel at a time.\n",
    "* Zero-padding ($P$): The amount of zeros which will be added around the border of the input volume.\n",
    "* Receptive field size ($F$): \n",
    "\n",
    "An important formula used often to design architectures and how many neurons fit the Conv Layer is given by \n",
    "\n",
    "$V = (W−F+2P)/S+1$\n",
    "\n",
    "where $V$ is the Output Volume Size and $W$ is the Input Size. They must be integers.\n",
    "\n",
    "![Convolution](http://xrds.acm.org/blog/wp-content/uploads/2016/06/Figure_5.png)\n",
    "\n",
    "Figure 1. Illustration of the operations in a convolutional layer. [Source](https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-cnns-illustrated-explanation/)\n",
    "\n",
    "This [calculator](https://madebyollin.github.io/convnet-calculator/) may help when designing ConvNet layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZelZxM9jgBDH"
   },
   "source": [
    "\n",
    "#### Pooling Layer\n",
    "\n",
    "Its function is to progressively reduce the size of the volume representation, the amount of parameters and computations in the network. Hence, they are key to control overfitting. It is common to periodically insert a Pooling layer in-between successive Conv layers.\n",
    "\n",
    "Their design Requires two hyperparameters:\n",
    "\n",
    "* Spatial extent ($F$)\n",
    "* Stride ($S$)\n",
    "\n",
    "In practice, the Max Pooling Layer has proven to work better than the Average and L2-norm one. There are only two commonly seen variations of the max pooling layer: a pooling layer with $F=3$, $S=2$ (also called *overlapping pooling*), and more commonly $F=2$, $S=2$.\n",
    "\n",
    "![Pooling](https://cdn-media-1.freecodecamp.org/images/96HH3r99NwOK818EB9ZdEbVY3zOBOYJE-I8Q).\n",
    "\n",
    "Figure 2. Illustration of the operation done by a pooling layer in a 2D representation. [Source](https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/)\n",
    "\n",
    "---\n",
    "\n",
    "If you are not familiar with one of these concepts, jump to this [tutorial](http://cs231n.github.io/convolutional-networks/) as a reference and check tout his [tool](http://setosa.io/ev/image-kernels/) about image kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O1Joiq9-WUJF"
   },
   "source": [
    "### Layer Patterns\n",
    "\n",
    "The most common form of a ConvNet architecture stacks a few CONV-RELU layers, follows them with POOL layers, and repeats this pattern until the image has been merged spatially to a small size. In the end, it is common to transition to fully-connected layers. The last fully-connected layer holds the output, such as the class scores. Some examples are listed below.\n",
    "\n",
    "INPUT -> CONV -> RELU -> FC\n",
    "\n",
    "INPUT -> [CONV -> RELU -> POOL]*2 -> FC -> RELU -> FC\n",
    "\n",
    "INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL]*3 -> [FC -> RELU]*2 -> FC\n",
    "\n",
    "\n",
    "![LeNet](https://miro.medium.com/max/1221/1*aQA7LuLJ2YfozSJa0pAO2Q.png)\n",
    "\n",
    "Figure 3. Illustration of the first convolutional network, LeNet-5.  [Source](https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ijuWsqWDhk6p"
   },
   "source": [
    "<!-- ## Rules of Thumb for designing your own CNN Architecture\n",
    "\n",
    "Observing the presented CNNs, we can observe some patterns and suggest some rules of thumb for designing conv nets.\n",
    "\n",
    "For instance, the input layer (that contains the image) should be divisible by 2 many times. Common numbers include 32 (e.g. CIFAR-10), 64, 96 (e.g. STL-10), or 224 (e.g. common ImageNet ConvNets), 384, and 512.\n",
    "\n",
    "The conv layers should be using small filters (e.g. 3x3 or at most 5x5), using a stride of S=1, and crucially, padding the input volume with zeros in such way that the conv layer does not alter the spatial dimensions of the input. That is, when F=3, then using P=1 will retain the original size of the input. When F=5, P=2. For a general F, it can be seen that P=(F−1)/2 preserves the input size. If you must use bigger filter sizes (such as 7x7 or so), it is only common to see this on the very first conv layer that is looking at the input image.\n",
    "\n",
    "The pool layers are in charge of downsampling the spatial dimensions of the input. The most common setting is to use max-pooling with 2x2 receptive fields (i.e. F=2), and with a stride of 2 (i.e. S=2). Note that this discards exactly 75% of the activations in an input volume (due to downsampling by 2 in both width and height). Another slightly less common setting is to use 3x3 receptive fields with a stride of 2, but this makes. It is very uncommon to see receptive field sizes for max pooling that are larger than 3 because the pooling is then too lossy and aggressive. This usually leads to worse performance.\n",
    "\n",
    "---\n",
    "\n",
    "[Great ressource for experimenting your architecture](https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html) -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QNGSx62UjE0y"
   },
   "source": [
    "## II. Benckmark and Deep Learning Challenge\n",
    "\n",
    "### [**ImageNet Large Scale Visual Recognition Challenge (ILSVRC)**](http://www.image-net.org/challenges/LSVRC/)\n",
    "\n",
    "The ImageNet project is a large visual database designed for use in visual object recognition software research. More than 14 million images have been hand-annotated by the project to indicate what objects are pictured and in at least one million of the images, bounding boxes are also provided. ImageNet contains more than 20,000 categories. Since 2010, the ImageNet project runs an annual software contest, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), where software programs compete to correctly classify and detect objects and scenes. The challenge uses a \"trimmed\" list of one thousand non-overlapping classes. [1]\n",
    "\n",
    "### Significance for deep learning\n",
    "\n",
    "On 30 September 2012, a convolutional neural network (CNN) called AlexNet achieved a top-5 error of 15.3% in the ImageNet 2012 Challenge, more than 10.8 percentage points lower than that of the runner up. This was made feasible due to the use of Graphics processing units (GPUs) during training, an essential ingredient of the deep learning revolution. According to The Economist, \"Suddenly people started to pay attention, not just within the AI community but across the technology industry as a whole.\" [1]\n",
    "\n",
    "---\n",
    "1. [Wikipedia article about ImageNet](https://en.wikipedia.org/wiki/ImageNet)\n",
    "\n",
    "2. [ImageNet dataset classes](https://github.com/Lasagne/Recipes/blob/master/examples/resnet50/imagenet_classes.txt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rn-qtXkOicY7"
   },
   "source": [
    "## II. Famous Architectures\n",
    "\n",
    "[**LeNet**](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf): The first successful applications of Convolutional Networks were developed by Yann LeCun in 1990's. \n",
    "\n",
    "LeNet-5 represents one of the simplest architectures. It has 2 convolutionary and 3 fully connected layers (thus \"5\" — it is very common to derive the names of neural networks from the number of convolutionary and fully connected layers they possess). The architecture has a number parameters of about 60,000.\n",
    "\n",
    "This architecture has become the traditional 'template': the stacking of convolutions and pooling layers, and the termination of the network with one or more fully-connected layers.\n",
    "\n",
    "[**AlexNet**](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf): The first work that popularized Convolutional Networks in Computer Vision was the AlexNet, developed by Alex Krizhevsky, Ilya Sutskever and Geoff Hinton. The AlexNet was submitted to the ImageNet ILSVRC challenge in 2012 and significantly outperformed the second runner-up.\n",
    "\n",
    "AlexNet has 8 layers — 5 convolutionary and 3 fully connected — with 60 M parameters. AlexNet only piled onto LeNet-5 a few more layers.\n",
    "\n",
    "They were the first to implement Rectified Linear Units (ReLUs) as activation functions.\n",
    "\n",
    "![](https://miro.medium.com/max/1564/1*2DT1bjmvC-U-lrL7tpj6wg.png)\n",
    "Figure 4. Illustration of the AlexNet architecture.\n",
    "\n",
    "[**GoogLeNet/Inception**](https://arxiv.org/pdf/1409.4842.pdf): The ILSVRC 2014 winner was a Convolutional Network from Szegedy et al. from Google. \n",
    "\n",
    "The network used a LeNet-inspired CNN but added a novel feature that is called an inception module. It used normalization of the array, distortions of images and RMSprop. To significantly reduce the number of parameters, this module is based on several very small convolutions.\n",
    "\n",
    "Its architecture consists of a 22-layer deep CNN but reduced the number of parameters from 60 million (AlexNet) to 4 million. A illustration of a variation of the Inception net is in the follwing sub-sections.\n",
    "\n",
    "[**VGGNet**](https://arxiv.org/pdf/1409.1556.pdf): The runner-up in ILSVRC 2014 was the network from Karen Simonyan and Andrew Zisserman that became known as the VGGNet.\n",
    "\n",
    "Knwoing that the best way to improve the efficiency of a deep neural networks is by increasing its scale (Szegedy et. al), the Visual Geometry Group (VGG) people created the VGG-16. It has 13 convolutionary and 3 fully connected layers, taking AlexNet's ReLU tradition with them. This network stacks on AlexNet more layers and uses filters of smaller sizes (2x2 and 3x3).\n",
    "\n",
    "[**ResNet**](https://arxiv.org/pdf/1603.05027.pdf): The Residual Networks was developed by Kaiming He et al. and was the winner of the ILSVRC 2015.\n",
    "\n",
    "We've seen nothing but an increasing number of layers in the design from the past few CNNs, and achieving better results. Yet \"with increasing network size, accuracy gets saturated (which might be unsurprising) and then degrades quickly.\" Microsoft Research people addressed this issue with ResNet — using skip connections (i.e. shortcut connections).\n",
    "\n",
    "ResNets' basic building block is the conv and identity blocks. In total it has 26M parameters. In terms of contribution, they popularised skip connections, designed even deeper CNNs without losing the generalization capacity of the model (up to 152 layers) and were among the first to use batch normalization.\n",
    "\n",
    "![Skip-connection](https://miro.medium.com/max/713/1*D0F3UitQ2l5Q0Ak-tjEdJg.png)\n",
    "\n",
    "Figure 5. An example of a skip connection in a single residual block. [Source](https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec)\n",
    "\n",
    "[**MobileNet**](https://arxiv.org/pdf/1704.04861.pdf): MobileNet was created by Google. Depthwise Separable Convolution is used to reduce the model size and complexity. It is particularly useful for mobile and embedded vision applications.\n",
    "\n",
    "A Depthwise Separable Convolution deals not just with the spatial dimensions, but with the depth dimension — the number of channels — as well. Sometimes it is easier to see than to say. The two following images illustrates normal convolution and depth wise convolution, respectively.\n",
    "\n",
    "![normal-conv](https://miro.medium.com/max/1523/1*fgYepSWdgywsqorf3bdksg.png)\n",
    "\n",
    "Figure 6.  A normal convolution with 8x8x1 output. [Source](https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728)\n",
    "\n",
    "![Depth-wise-conv](https://miro.medium.com/max/1635/1*yG6z6ESzsRW-9q5F_neOsg.png)\n",
    "\n",
    "Figure 7. Depthwise convolution, uses 3 kernels to transform a 12x12x3 image to a 8x8x3 image. [Source](https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728)\n",
    "\n",
    "By separating the convolution in two parts, it has a smaller model size (fewer number of parameters) and a smaller complexity (fewer multiplications and additions). When 1.0 MobileNet-224 is used, it outperforms GoogLeNet (Winner of ILSVRC 2014) and VGGNet (1st Runner Up of ILSVRC 2014) while the multi-adds and parameters are much fewer. The two part spatial separable convolution is illustrated below.\n",
    "\n",
    "![](https://miro.medium.com/max/1548/1*o3mKhG3nHS-1dWa_plCeFw.png)\n",
    "\n",
    "Figure 8. Simple and spatial separable convolution. 9 operations vs 6 operations. [Source](https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we will compare in more detail the following CNNs architectures:\n",
    "\n",
    "* Variation of the **VGGNet** (VGG-16)\n",
    "* Variation of the **ResNet** (ResNet-50)\n",
    "* Variation of the **GoogLeNet/Inception** (Inception-ResNet-V2)\n",
    "\n",
    "And train a **MobileNetV2** to classify dogs and cats with feature extraction.\n",
    "\n",
    "Note the amount of trainable parameters in the networks. To train it on a simple machine would take ... $\\infty$. That is why we are going to load a pre-trained weights and then do the predictions.\n",
    "\n",
    "**Let's code!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YRd605MQp-GP"
   },
   "source": [
    "#### Load example pictures to test the CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "VZClg9RZuCpm",
    "outputId": "bedb58dd-45f3-46e7-8feb-818aed51281a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading https://files.pythonhosted.org/packages/19/0d/b97f3c4f14b7f04a741e91133b099ff9b393a977d7dbb4a1236e50b97827/gdown-3.9.1.tar.gz\n",
      "Collecting filelock\n",
      "  Downloading https://files.pythonhosted.org/packages/93/83/71a2ee6158bb9f39a90c0dea1637f81d5eef866e188e1971a1b1ab01a35a/filelock-3.0.12-py3-none-any.whl\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\tf\\lib\\site-packages (from gdown) (2.22.0)\n",
      "Requirement already satisfied: six in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\tf\\lib\\site-packages (from gdown) (1.13.0)\n",
      "Collecting tqdm\n",
      "  Downloading https://files.pythonhosted.org/packages/72/c9/7fc20feac72e79032a7c8138fd0d395dc6d8812b5b9edf53c3afd0b31017/tqdm-4.41.1-py2.py3-none-any.whl (56kB)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\tf\\lib\\site-packages (from requests[socks]->gdown) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\tf\\lib\\site-packages (from requests[socks]->gdown) (1.25.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\tf\\lib\\site-packages (from requests[socks]->gdown) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\tf\\lib\\site-packages (from requests[socks]->gdown) (2019.11.28)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6; extra == \"socks\"\n",
      "  Downloading https://files.pythonhosted.org/packages/8d/59/b4572118e098ac8e46e399a1dd0f2d85403ce8bbaad9ec79373ed6badaf9/PySocks-1.7.1-py3-none-any.whl\n",
      "Building wheels for collected packages: gdown\n",
      "  Building wheel for gdown (setup.py): started\n",
      "  Building wheel for gdown (setup.py): finished with status 'done'\n",
      "  Created wheel for gdown: filename=gdown-3.9.1-cp37-none-any.whl size=9021 sha256=c13690245559be88b78295a7d85464c660f307b12667665efc773b65968c7ed7\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\pip\\Cache\\wheels\\b5\\35\\4a\\c307fa6af0330a6f82a7072d005647b132b7dce14c7b02fed0\n",
      "Successfully built gdown\n",
      "Installing collected packages: filelock, tqdm, gdown, PySocks\n",
      "Successfully installed PySocks-1.7.1 filelock-3.0.12 gdown-3.9.1 tqdm-4.41.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ZsZZgzC1ppe02x2AvVw6b0dmTDFVEYH6\n",
      "To: C:\\Users\\user\\Desktop\\GitHub\\CNN-architectures-comparison\\examples.zip\n",
      "\n",
      "  0%|          | 0.00/439k [00:00<?, ?B/s]\n",
      "100%|##########| 439k/439k [00:00<00:00, 7.55MB/s]\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Load an example picture from Google Drive\n",
    "!pip install gdown\n",
    "# Examples\n",
    "!gdown --id 1ZsZZgzC1ppe02x2AvVw6b0dmTDFVEYH6\n",
    "!unzip examples.zip > output1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CxIx8DkSFHvv"
   },
   "source": [
    "### VGG-16\n",
    "\n",
    "In this section we will load the VGG-16 convnet with the pre-trained weights and classifier (include_top) and try to classify a picture example. \n",
    "\n",
    "![](https://miro.medium.com/max/1799/1*_vGloND6yyxFeFH5UyCDVg.png)\n",
    "Figure 9. Illustration of the VGG-16 architecture based on the original paper. [Source](https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d#6872)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 941
    },
    "colab_type": "code",
    "id": "YeRu9lCwD8dB",
    "outputId": "702514d8-fd58-4691-96b4-5cf79d3ace58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\user\\appdata\\local\\programs\\python\\python37\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "397574144/553467096 [====================>.........] - ETA: 35s"
     ]
    }
   ],
   "source": [
    "# Ignore tf versioning warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# VGG\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "# Load the model with the top classifier\n",
    "vgg16 = VGG16(weights='imagenet',\n",
    "                  include_top=True,\n",
    "                  input_shape=(224, 224, 3),\n",
    "                  classes=1000)\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "jFO6YuQ1ghm8",
    "outputId": "7418c304-75ab-4e91-9807-4f65eaf4ac91"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "# Pre proccess the input and predic result\n",
    "def predict_example(model, directory='',filename='beluga.jpg', name=\"\"):\n",
    "    # load an image from file\n",
    "    figure = load_img(directory+filename, target_size=(224,224))\n",
    "    # convert the image pixels to a numpy array\n",
    "    image = img_to_array(figure)\n",
    "    # reshape data for the model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # prepare the image for the VGG model\n",
    "    image = preprocess_input(image)\n",
    "    # predict the probability across all output classes\n",
    "    yhat = model.predict(image)\n",
    "    # convert the probabilities to class labels\n",
    "    label = decode_predictions(yhat)\n",
    "    # retrieve the most likely result, e.g. highest probability\n",
    "    label = label[0][0]\n",
    "    # print the classification\n",
    "    print(name, '%s (%.2f%%)' % (label[1], label[2]*100))\n",
    "    return figure\n",
    "\n",
    "# Your time to play\n",
    "predict_example(vgg16, filename='beluga.jpg')\n",
    "# There are over 1000 different classes available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-xQ_h52bU24"
   },
   "source": [
    "### ResNet-50\n",
    "\n",
    "We will do the same thing for the ResNet-50.\n",
    "\n",
    "![altext](https://miro.medium.com/max/2368/1*zbDxCB-0QDAc4oUGVtg3xw.png)\n",
    "\n",
    "Figure 10. Illustration of the ResNet-50 architecture based on the GitHub [code](https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet_common.py) from keras-team. [Source](https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d#e4b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Epww80eR2aQs",
    "outputId": "7af15a4e-8826-4a73-be62-c79200548ede"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "# ResNet\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "resnet50 = ResNet50(weights='imagenet',\n",
    "                include_top=True,\n",
    "                input_shape=(224, 224, 3),\n",
    "                classes=1000)\n",
    "resnet50.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "rYV_SH-PjZ5i",
    "outputId": "c9038f65-54ea-4864-dd67-c718b55fead9"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "# Your time to play\n",
    "predict_example(resnet50, filename='beluga.jpg')\n",
    "# Remember, there are over 1000 available classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p1UflYbraydD"
   },
   "source": [
    "### Inception-ResNet-V2\n",
    "\n",
    "And the same for the variation of the Inception network.\n",
    "\n",
    "![alttext](https://miro.medium.com/max/3125/1*xpb6QFQ4IknSmxmgai8w-Q.png)\n",
    "\n",
    "Figure 11. Illustration of the Inception-ResNet-V2 architecture. [Source](https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d#e4b1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6mmFKisnax8F",
    "outputId": "568c1f5f-1646-4125-a223-bd18d5ec4c25"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "# GoogleLeNet/Inception\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "inception = InceptionResNetV2(weights='imagenet',\n",
    "                include_top=True,\n",
    "                input_shape=(299, 299, 3), # The input shape for this convnet is different\n",
    "                classes=1000)\n",
    "inception.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "colab_type": "code",
    "id": "dwkfueesnkU4",
    "outputId": "b90a4ee2-04fe-4cf2-c60b-9a861d270879"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input, decode_predictions\n",
    "def predict_example_inception(model, directory='',filename='2.jpg', name=\"\"):\n",
    "    figure = load_img(directory+filename, target_size=(299,299))\n",
    "    image = img_to_array(figure)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = preprocess_input(image)\n",
    "    yhat = model.predict(image)\n",
    "    label = decode_predictions(yhat)[0][0]\n",
    "    print(name, '%s (%.2f%%)' % (label[1], label[2]*100))\n",
    "    return figure\n",
    "predict_example_inception(inception,filename='beluga.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sxm1xSj4rcjS"
   },
   "source": [
    "#### Playground\n",
    "\n",
    "Feel free to upload your own pictures and compare single example performance between the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "colab_type": "code",
    "id": "Epkd1Hlgre2e",
    "outputId": "8e51d590-9e6c-4165-e736-80afb41a8604"
   },
   "outputs": [],
   "source": [
    "filename = 'truck.jpg'\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "predict_example(vgg16, directory=\"\", filename=filename, name=\"VGG-16\", )\n",
    "predict_example(resnet50, directory=\"\", filename=filename, name=\"ResNet-50\")\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input, decode_predictions\n",
    "predict_example_inception(inception, directory=\"\", filename=filename, name=\"InceptionResNetV2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0_g_J6DEExeT"
   },
   "source": [
    "## Comparison of Principal Architectures\n",
    "\n",
    "The following table show some data for several state of the art CNN architectures. This data is made available by the Keras team in [their documentation](https://keras.io/applications/). We will use this data to create graphs and drive conclusions from the different architectures. \n",
    "\n",
    "<hr />\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Model</th>\n",
    "<th align=\"right\">Size</th>\n",
    "<th align=\"right\">Top-1 Accuracy</th>\n",
    "<th align=\"right\">Top-5 Accuracy</th>\n",
    "<th align=\"right\">Parameters</th>\n",
    "<th align=\"right\">Depth</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>AlexNet </td>\n",
    "<td align=\"right\">60 MB</td>\n",
    "<td align=\"right\">0.633</td>\n",
    "<td align=\"right\">0.847</td>\n",
    "<td align=\"right\">60,954,656</td>\n",
    "<td align=\"right\">8</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Xception </td>\n",
    "<td align=\"right\">88 MB</td>\n",
    "<td align=\"right\">0.790</td>\n",
    "<td align=\"right\">0.945</td>\n",
    "<td align=\"right\">22,910,480</td>\n",
    "<td align=\"right\">126</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>VGG16 </td>\n",
    "<td align=\"right\">528 MB</td>\n",
    "<td align=\"right\">0.713</td>\n",
    "<td align=\"right\">0.901</td>\n",
    "<td align=\"right\">138,357,544</td>\n",
    "<td align=\"right\">23</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>VGG19 </td>\n",
    "<td align=\"right\">549 MB</td>\n",
    "<td align=\"right\">0.713</td>\n",
    "<td align=\"right\">0.900</td>\n",
    "<td align=\"right\">143,667,240</td>\n",
    "<td align=\"right\">26</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ResNet50 </td>\n",
    "<td align=\"right\">98 MB</td>\n",
    "<td align=\"right\">0.749</td>\n",
    "<td align=\"right\">0.921</td>\n",
    "<td align=\"right\">25,636,712</td>\n",
    "<td align=\"right\">50</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ResNet101 </td>\n",
    "<td align=\"right\">171 MB</td>\n",
    "<td align=\"right\">0.764</td>\n",
    "<td align=\"right\">0.928</td>\n",
    "<td align=\"right\">44,707,176</td>\n",
    "<td align=\"right\">101</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ResNet152 </td>\n",
    "<td align=\"right\">232 MB</td>\n",
    "<td align=\"right\">0.766</td>\n",
    "<td align=\"right\">0.931</td>\n",
    "<td align=\"right\">60,419,944</td>\n",
    "<td align=\"right\">512</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ResNet50V2 </td>\n",
    "<td align=\"right\">98 MB</td>\n",
    "<td align=\"right\">0.760</td>\n",
    "<td align=\"right\">0.930</td>\n",
    "<td align=\"right\">25,613,800</td>\n",
    "<td align=\"right\">50</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ResNet101V2 </td>\n",
    "<td align=\"right\">171 MB</td>\n",
    "<td align=\"right\">0.772</td>\n",
    "<td align=\"right\">0.938</td>\n",
    "<td align=\"right\">44,675,560</td>\n",
    "<td align=\"right\">101</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ResNet152V2 </td>\n",
    "<td align=\"right\">232 MB</td>\n",
    "<td align=\"right\">0.780</td>\n",
    "<td align=\"right\">0.942</td>\n",
    "<td align=\"right\">60,380,648</td>\n",
    "<td align=\"right\">512</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>InceptionV3 </td>\n",
    "<td align=\"right\">92 MB</td>\n",
    "<td align=\"right\">0.779</td>\n",
    "<td align=\"right\">0.937</td>\n",
    "<td align=\"right\">23,851,784</td>\n",
    "<td align=\"right\">159</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>InceptionResNetV2 </td>\n",
    "<td align=\"right\">215 MB</td>\n",
    "<td align=\"right\">0.803</td>\n",
    "<td align=\"right\">0.953</td>\n",
    "<td align=\"right\">55,873,736</td>\n",
    "<td align=\"right\">572</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MobileNet </td>\n",
    "<td align=\"right\">16 MB</td>\n",
    "<td align=\"right\">0.704</td>\n",
    "<td align=\"right\">0.895</td>\n",
    "<td align=\"right\">4,253,864</td>\n",
    "<td align=\"right\">88</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MobileNetV2 </td>\n",
    "<td align=\"right\">14 MB</td>\n",
    "<td align=\"right\">0.713</td>\n",
    "<td align=\"right\">0.901</td>\n",
    "<td align=\"right\">3,538,984</td>\n",
    "<td align=\"right\">88</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>DenseNet121 </td>\n",
    "<td align=\"right\">33 MB</td>\n",
    "<td align=\"right\">0.750</td>\n",
    "<td align=\"right\">0.923</td>\n",
    "<td align=\"right\">8,062,504</td>\n",
    "<td align=\"right\">121</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>DenseNet169 </td>\n",
    "<td align=\"right\">57 MB</td>\n",
    "<td align=\"right\">0.762</td>\n",
    "<td align=\"right\">0.932</td>\n",
    "<td align=\"right\">14,307,880</td>\n",
    "<td align=\"right\">169</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>DenseNet201 </td>\n",
    "<td align=\"right\">80 MB</td>\n",
    "<td align=\"right\">0.773</td>\n",
    "<td align=\"right\">0.936</td>\n",
    "<td align=\"right\">20,242,984</td>\n",
    "<td align=\"right\">201</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>NASNetMobile </td>\n",
    "<td align=\"right\">23 MB</td>\n",
    "<td align=\"right\">0.744</td>\n",
    "<td align=\"right\">0.919</td>\n",
    "<td align=\"right\">5,326,716</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>NASNetLarge </td>\n",
    "<td align=\"right\">343 MB</td>\n",
    "<td align=\"right\">0.825</td>\n",
    "<td align=\"right\">0.960</td>\n",
    "<td align=\"right\">88,949,818</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<hr />\n",
    "\n",
    "**Size:** size of the network in memory.\n",
    "\n",
    "**Top-1 Accuracy:** check if the top class (the one having the highest probability) is the same as the target label.\n",
    "\n",
    "**Top-5 Accuracy:** check if the target label is one of your top 5 predictions (the 5 ones with the highest probabilities).\n",
    "\n",
    "In both cases, the top score is computed as the times a predicted label matched the target label, divided by the number of data-points evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "dwAHsAzr0yWk",
    "outputId": "6ffd9560-b641-420b-9c5e-a7a07256210b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "names = np.array(['AlexNet','Xception','VGG16','VGG19','ResNet50','ResNet101','ResNet152','ResNet50V2','ResNet101V2','ResNet152V2','InceptionV3','InceptionResNetV2','MobileNet','MobileNetV2','DenseNet121','DenseNet169','DenseNet201','NASNetMobile','NASNetLarge'])\n",
    "sizes = np.array([60,88,528,549,98,171,232,98,171,232,92,215,16,14,33,57,80,23,343])\n",
    "top_1_acc = np.array([0.633,0.79,0.713,0.713,0.749,0.764,0.766,0.76,0.772,0.78,0.779,0.803,0.704,0.713,0.75,0.762,0.773,0.744,0.825])\n",
    "top_5_acc = np.array([0.847,0.945,0.901,0.9,0.921,0.928,0.931,0.93,0.938,0.942,0.937,0.953,0.895,0.901,0.923,0.932,0.936,0.919,0.96])\n",
    "params = np.array([60954656,22910480,138357544,143667240,25636712,44707176,60419944,25613800,44675560,60380648,23851784,55873736,4253864,3538984,8062504,14307880,20242984,5326716,88949818])\n",
    "top_1_acc_density = top_1_acc/params*10**6\n",
    "colors = [np.random.rand(3,) for i in range(0,len(names))]\n",
    "df = pd.DataFrame(data={'Names': names, 'Size (MB)':sizes, 'top_1_acc':top_1_acc, \n",
    "                        'top_5_acc':top_5_acc, 'Parameters':params, 'top_1_acc_density':top_1_acc_density,'col':colors})\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2xDxd8H64wR1",
    "outputId": "9fa49f9e-00a7-4194-fca9-a81860ed881e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig, ax = plt.subplots(3,2,figsize=(16,16))\n",
    "plt.subplots_adjust(hspace=0.7)\n",
    "ax[0][0].set_title('Top 1 Accuracy')\n",
    "plt.sca(ax[0][0])\n",
    "plt.ylim([0.5,0.9])\n",
    "g = sns.barplot(x='Names', y='top_1_acc', data=df.sort_values('top_1_acc'), palette=df.sort_values('top_1_acc')['col'], ax=ax[0][0])\n",
    "g = g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
    "ax[0][1].set_title('Top 5 Accuracy')\n",
    "plt.sca(ax[0][1])\n",
    "plt.ylim([0.6,1])\n",
    "g = sns.barplot(x='Names', y='top_5_acc', data=df.sort_values('top_5_acc'),  palette=df.sort_values('top_5_acc')['col'], ax=ax[0][1])\n",
    "g = g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
    "ax[1][0].set_title('Number of Parameters')\n",
    "g = sns.barplot(x='Names', y='Parameters', data=df.sort_values('Parameters'),palette=df.sort_values('Parameters')['col'], ax=ax[1][0])\n",
    "g = g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
    "ax[1][1].set_title('Size of the network in MB')\n",
    "g = sns.barplot(x='Names', y='Size (MB)', data=df.sort_values('Size (MB)'), palette=df.sort_values('Size (MB)')['col'], ax=ax[1][1])\n",
    "g = g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
    "ax[2][0].set_title('Top 1 accuracy density [%/M-Params]')\n",
    "g = sns.barplot(x='Names', y='top_1_acc_density', data=df.sort_values('top_1_acc_density'), palette=df.sort_values('top_1_acc_density')['col'], ax=ax[2][0])\n",
    "g = g.set_xticklabels(g.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TmltFSv5cqj3"
   },
   "source": [
    "## Feature Extraction (Optional)\n",
    "\n",
    "In this section we will use all the power of Keras application to train a state of the art cnn to classify cats and dogs. For doing so we will use a technique called Feature Extraction.\n",
    "\n",
    "Keras Applications are deep learning models that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning. Go to [their documentation](https://keras.io/applications/) for reference.\n",
    "\n",
    "Since the imagenet dataset is too big, with more than 150GB and 1000 classes, we are going to adapt the CNNs from Keras Application to distinguish cats and dogs on our own dataset. For doing this, we will conserve the convolutinal base of the networks and train a novel classifier.\n",
    "\n",
    "![alttext](https://camo.githubusercontent.com/2de162f6f3587a0422389355909594413ae17742/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f626f6f6b2e6b657261732e696f2f696d672f6368352f7377617070696e675f66635f636c61737369666965722e706e67)\n",
    "\n",
    "Figure 12. Illustration of how a feature extraction application work in keras. The model consists of a \"convolutional base\" and a \"top\" part where there is the densely connected classifier for 1000 different classes.\n",
    "\n",
    "---\n",
    "\n",
    "This subsection is based on [this](https://www.tensorflow.org/tutorials/images/transfer_learning) notebook from François Chollet, developper of Keras.\n",
    "\n",
    "This part is preferably run in Google Colab, since it requires tensorflow 2.x.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mdIBjScFFnfq"
   },
   "source": [
    "### Loaf tensorflow dataset cats_vs_dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-pC8Ik5cENjN"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGRR3ZLrENjP"
   },
   "outputs": [],
   "source": [
    "SPLIT_WEIGHTS = (8, 1, 1)\n",
    "splits = tfds.Split.TRAIN.subsplit(weighted=SPLIT_WEIGHTS)\n",
    "\n",
    "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
    "    'cats_vs_dogs', split=list(splits),\n",
    "    with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "k7bxP562GiHd",
    "outputId": "16c31566-f754-4941-e83c-feff49c55ce7"
   },
   "outputs": [],
   "source": [
    "print(raw_train)\n",
    "print(raw_validation)\n",
    "print(raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "yFtdfo33ENjR",
    "outputId": "f0159283-ab69-4bd0-d0b0-349da5ff2173"
   },
   "outputs": [],
   "source": [
    "get_label_name = metadata.features['label'].int2str\n",
    "\n",
    "for image, label in raw_train.take(2):\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    plt.title(get_label_name(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "45_pzwU6ENjW"
   },
   "source": [
    "### Format the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HYS2jouxENjW"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 160 # All images will be resized to 160x160\n",
    "\n",
    "def format_example(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image/127.5) - 1\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jdngHMiWENjZ"
   },
   "source": [
    "Apply this function to each item in the dataset using the map method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9KfwlB0RENja"
   },
   "outputs": [],
   "source": [
    "train = raw_train.map(format_example)\n",
    "validation = raw_validation.map(format_example)\n",
    "test = raw_test.map(format_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_0XrUkW5ENjd"
   },
   "source": [
    "Now shuffle and batch the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H5Jy7pyWENjd"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X7CQ08MyENjg"
   },
   "outputs": [],
   "source": [
    "train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "validation_batches = validation.batch(BATCH_SIZE)\n",
    "test_batches = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PJe3dMfCENji",
    "outputId": "9b484a3f-f89a-4036-8bc1-a370b25305aa"
   },
   "outputs": [],
   "source": [
    "for image_batch, label_batch in train_batches.take(1):\n",
    "   pass\n",
    "\n",
    "image_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eUVAn6tVENjk"
   },
   "outputs": [],
   "source": [
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "# Create the base model from the pre-trained model MobileNet V2 (Convolutional base)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5sK-_j0MENjl"
   },
   "source": [
    "This feature extractor converts each `160x160x3` image into a `5x5x1280` block of features. This block then will be inputed in our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7RtcIEs9ENjl",
    "outputId": "56770520-6761-48e7-e115-2ebf61387eb9"
   },
   "outputs": [],
   "source": [
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xKyma7y_ENjn"
   },
   "source": [
    "### Freeze the convolutional base\n",
    "\n",
    "It is important to freeze the convolutional base before you compile and train the model. Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training. MobileNet V2 has many layers, so setting the entire model's trainable flag to False will freeze all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QLi20-ZzENjn",
    "outputId": "8b53045b-9b08-4d72-bdad-80a5615f1e61"
   },
   "outputs": [],
   "source": [
    "base_model.trainable = False\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nlDiJFMyENjr"
   },
   "source": [
    "### Add a classification top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8S2fM__zENjs"
   },
   "source": [
    "To generate predictions from the block of features, average over the spatial `5x5` spatial locations, using a `tf.keras.layers.GlobalAveragePooling2D` layer to convert the features to  a single 1280-element vector per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PglR7xzrENju",
    "outputId": "cd0f48b7-d624-451c-9c2e-05fd908e8972"
   },
   "outputs": [],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AzWhkiDCENjx"
   },
   "source": [
    "Apply a `tf.keras.layers.Dense` layer to convert these features into a single prediction per image. You don't need an activation function here because this prediction will be treated as a `logit`, or a raw prediction value.  Positive numbers predict class 1, negative numbers predict class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FOcJGVpwENjx",
    "outputId": "9b67b55f-ed09-4b3a-fb00-57427ae62e52"
   },
   "outputs": [],
   "source": [
    "prediction_layer = tf.keras.layers.Dense(1)\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GQN_nlY6ENj3"
   },
   "source": [
    "Now stack the feature extractor, and these two layers using a `tf.keras.Sequential` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dbYBOn1eENj4"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  global_average_layer,\n",
    "  prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ci5pD1t-ENj6"
   },
   "source": [
    "### Compile the model\n",
    "\n",
    "You must compile the model before training it.  Since there are two classes, use a binary cross-entropy loss with `from_logits=True` since the model provides a linear output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "30vM-dM3ENj7",
    "outputId": "ea61b8b1-19be-4fbb-e802-3581065c3bfd"
   },
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uUPxloRjENj_"
   },
   "source": [
    "The 2.5M parameters in MobileNet are frozen, but there are 1.2K _trainable_ parameters in the Dense layer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TJgPH34vENkB"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "After training for 10 epochs, you should see ~96% accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "44B-dMhqENkE",
    "outputId": "194ea389-4af7-44a3-b3ba-813e3bb08e57"
   },
   "outputs": [],
   "source": [
    "num_train, num_val, num_test = (\n",
    "    metadata.splits['train'].num_examples*weight/10\n",
    "    for weight in SPLIT_WEIGHTS\n",
    ")\n",
    "initial_epochs = 10\n",
    "steps_per_epoch = round(num_train)//BATCH_SIZE\n",
    "validation_steps=20\n",
    "loss0,accuracy0 = model.evaluate(validation_batches, steps = validation_steps)\n",
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial accuracy: {:.2f}\".format(accuracy0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "0dAdhWvCENkN",
    "outputId": "97f115c8-98c2-44d5-f8f6-09a5d253c765"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_batches,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "88q6US8VENkR"
   },
   "source": [
    "### Learning curves\n",
    "\n",
    "Let's take a look at the learning curves of the training and validation accuracy/loss when using the MobileNet V2 base model as a fixed feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "cTuAfTySENkR",
    "outputId": "b7ed5c87-9d48-4d69-d7f4-3651320ce4b6"
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8RFtW0ksFHSt"
   },
   "source": [
    "In addition you could perform a fine tuning in the weights of the convolutional layers, improving even more the performance of the CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lMJOFg78WT_O"
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "Instead of rolling your own architecture for a problem, you should look at whatever architecture currently works best on ImageNet, download a pretrained model and finetune it on your data. You should rarely ever have to train a ConvNet from scratch or design one from scratch.\n",
    "\n",
    "\n",
    "Convnets are the best type of machine learning models for computer vision tasks. It is possible to train one from scratch even on a very small dataset, with decent results.\n",
    "\n",
    "On a small dataset, overfitting will be the main issue. Data augmentation is a powerful way to fight overfitting when working with image data.\n",
    "\n",
    "It is easy to reuse an existing convnet on a new dataset, via feature extraction. This is a very valuable technique for working with small image datasets.\n",
    "\n",
    "As a complement to feature extraction, one may use fine-tuning, which adapts to a new problem some of the representations previously learned by an existing model. This pushes performance a bit further.\n",
    "\n",
    "Now you have a solid set of tools for dealing with image classification problems.\n",
    "\n",
    "---\n",
    "Further Reading:\n",
    "\n",
    "* [An Analysis of Deep Neural Network Models for Practical Applications](https://arxiv.org/pdf/1605.07678.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "25 - Comparison of CNN architectures.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
